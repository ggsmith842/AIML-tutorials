{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMB7ujgAn/rPMdq/5xGtDvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggsmith842/pytorch-tutorials/blob/main/BIDIRECTIONAL_LSTM_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq datasets transformers"
      ],
      "metadata": {
        "id": "kmLxRwpuG0uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pwrWB5UtwLCT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9mf7swdGKy1",
        "outputId": "67fab1fb-2198-4ae1-c66f-8c09dffcf60e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Stage Data"
      ],
      "metadata": {
        "id": "0Pm900TSrDad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load imdb dataset from huggingface\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")"
      ],
      "metadata": {
        "id": "rGXdjDxSHGNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset = concatenate_datasets([ds['train'], ds['test']])\n",
        "split_dataset = merged_dataset.train_test_split(test_size=0.25, shuffle=True, seed=42)\n",
        "\n",
        "train = split_dataset['train']\n",
        "test = split_dataset['test']"
      ],
      "metadata": {
        "id": "iY555jMIFqq1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "e6mDxYUrqSAL",
        "outputId": "a394db59-7f75-4c84-b083-410a62129688"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This movie was one of the most boring horror movies I have seen in a long time (and I have seen a lot). Personally I liked the piercing take on it all that was original but other than that it was pretty unwatchable. I could not stand Dee Snider as an actor nor as a singer. I seemed that he was trying with everything he said to make it a memorable quote, which they weren't. I can get movies for free and I still didn't think it was worth the time to get.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "qFsOhLFPrGfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "800s_obYrBAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_reviews(data: Dataset):\n",
        "  inputs = data['text']\n",
        "  tokens = tokenizer(\n",
        "      inputs,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=512)\n",
        "\n",
        "  return (tokens['input_ids'], tokens['attention_mask'])"
      ],
      "metadata": {
        "id": "PjMaR4PNs0n3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_mask = tokenize_reviews(train)\n",
        "test_input_ids, test_attention_mask = tokenize_reviews(test)"
      ],
      "metadata": {
        "id": "fEf5FfTPtksP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = torch.tensor(train['label'])\n",
        "test_labels = torch.tensor(test['label'])"
      ],
      "metadata": {
        "id": "HbciSmcr7AZo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation Loader"
      ],
      "metadata": {
        "id": "7UpX-yelrHvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom datasets\n",
        "train_dataset = TensorDataset(\n",
        "    train_input_ids,\n",
        "    train_attention_mask,\n",
        "    train_labels\n",
        "    )\n",
        "\n",
        "test_dataset = TensorDataset(\n",
        "    test_input_ids,\n",
        "    test_attention_mask,\n",
        "    test_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "K-5bvhp7_ZLs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)"
      ],
      "metadata": {
        "id": "C0OvWN0269Eq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Instantiation and Training"
      ],
      "metadata": {
        "id": "GYZGh6ev4pMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, embedding_dim: int, hidden_dim: int , output_dim: int, num_layers: int, dropout: float = 0.0):\n",
        "    super(LSTM, self).__init__()\n",
        "    vocab_size = len(tokenizer.get_vocab())\n",
        "    self.embedding = nn.Embedding.from_pretrained(torch.zeros(vocab_size,embedding_dim))\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    # dropout % to prevent overfitting\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    # fully connected (linear) layer; x2 because bidirectional\n",
        "    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    # pass input ids to get embeddings\n",
        "    embedded = self.embedding(input_ids)\n",
        "    outputs, _ = self.lstm(embedded)\n",
        "    outputs = self.dropout(outputs)\n",
        "\n",
        "    outputs = self.fc(outputs[:, -1, :])\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "Xkzgf48t9eMm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define training loop\n",
        "def training_loop(model: nn.Module, dataloader: DataLoader, epochs: int = 10):\n",
        "  model.train()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "      batch = [item.to(device, non_blocking=True) for item in batch]\n",
        "      input_ids, attention_mask, labels = batch\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted.squeeze() == labels.float()).sum().item()\n",
        "      loss = loss_func(outputs.squeeze(), labels.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch {1+epoch}/{epochs}: Loss {loss.item()} | Accuracy {accuracy*100:.2f}%\\n')\n"
      ],
      "metadata": {
        "id": "1rpiGkLTu27m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_loop(model: nn.Module, dataloader: DataLoader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(dataloader):\n",
        "      batch = [item.to(device) for item in batch]\n",
        "      input_ids, attention_mask, labels = batch\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted.squeeze() == labels.float()).sum().item()\n",
        "  accuracy = correct/ total\n",
        "\n",
        "  print(f'Accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "id": "iAU1Jgv21IDE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "num_layers = 2\n",
        "dropout = 0.5"
      ],
      "metadata": {
        "id": "3avARuqnxn2q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTM(embedding_dim=embedding_dim, output_dim=output_dim, num_layers=num_layers, hidden_dim=hidden_dim).to(device)\n",
        "optimizer = optim.Adam(lstm_model.parameters(), lr=0.003)\n",
        "loss_func = nn.BCEWithLogitsLoss().to(device)"
      ],
      "metadata": {
        "id": "0FUaHHk76Sdq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(lstm_model, train_dataloader, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7M-4iwJzggh",
        "outputId": "fdb6a04d-ab46-4e21-be16-0b5e60f873f9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:49<00:00, 10.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: Loss 0.6946743726730347 | Accuracy 50.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [02:05<00:00,  9.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10: Loss 0.6942399144172668 | Accuracy 50.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:58<00:00,  9.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10: Loss 0.6887026429176331 | Accuracy 50.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:53<00:00, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10: Loss 0.692821204662323 | Accuracy 50.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:46<00:00, 10.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10: Loss 0.6931650042533875 | Accuracy 49.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:45<00:00, 11.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10: Loss 0.6937905550003052 | Accuracy 49.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:45<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10: Loss 0.6930750608444214 | Accuracy 49.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:45<00:00, 11.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10: Loss 0.6940949559211731 | Accuracy 49.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:45<00:00, 11.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10: Loss 0.6928125023841858 | Accuracy 49.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [01:45<00:00, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10: Loss 0.6923677325248718 | Accuracy 50.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loop(lstm_model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbNxik732KEG",
        "outputId": "0a78cd9d-373f-4aa0-da8f-29472bed224b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:14<00:00, 27.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}